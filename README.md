<!-- ![Syn Eval Tool](./assets/syn-eval-tool.jpg) -->


<!-- <p align="center">
  <a href="https://huggingface.co/spaces/Speech-Arena-2025/Voice-Privacy-Arena">
    <img src="https://img.shields.io/badge/VP%20Arena-%F0%9F%A4%97-ffc107?color=ffc107&logoColor=white">
  </a>
  <a href="https://speech-arena.github.io/">
    <img src="https://img.shields.io/badge/Speech%20Arena%20Team-ğŸŒ-orange">
  </a>
  <br>
  <a href="https://github.com/Speech-Arena/SynEvalArena#">
    <img src="https://img.shields.io/badge/Synthetic%20Speech%20Eval-black?logo=github">
  </a>
  <a href="https://github.com/Speech-Arena/SynEvalArena#">
    <img src="https://img.shields.io/badge/Voice%20Anonymization%20Eval-black?logo=github">
  </a> -->

<!-- </p> -->

<!-- # SynEvalArena

Following our release of the (Voice Privacy Arena)[https://github.com/Speech-Arena/SynEvalArena#] leaderboard, Speech Arena Team presents accompanying SynEvalArena toolkit to evaluate your own systems. 
 


# Overview
Tradidional evaluation of voice anonymization systems focuses on metrics like EER (Equal Error Rate) and WER (Word Error Rate). In additon to these standard metrcis, the toolkit offers a suite of many others for robust evaluation of VA systems.

This reporistroy contains two main toolkit directories - 
- `synthetic_speech_eval` - Metrics for evaluation of synthetic speech generated by modern end to end voice anonymization and voice conversion systems. Check out the [Readme](./ss/Readme.md) for more details. 
- `voice_privacy_eval` - We propose two new metrcis - AUDI (Audio Utility Distortion Index) and Fusion EER to evaluate the utility and privacy of voice anonymization systems. Check the [Readme](./vp/Readme.md) for more details. 

 --> 

 ![ğŸ§ Syn Eval Tool](./assets/syn-eval-tool.jpg)
 <p align="center">
  <a href="https://huggingface.co/spaces/Speech-Arena-2025/Voice-Privacy-Arena">
    <img src="https://img.shields.io/badge/VP%20Arena-%F0%9F%A4%97-ffc107?color=ffc107&logoColor=white">
  </a>
  <a href="https://speech-arena.github.io/">
    <img src="https://img.shields.io/badge/Speech%20Arena%20Team-ğŸŒ-pink">
  </a>
  <br>
  <a href="https://github.com/Speech-Arena/SynEvalArena#">
    <img src="https://img.shields.io/badge/Synthetic%20Speech%20Eval-black?logo=github">
  </a>
  <a href="https://github.com/Speech-Arena/SynEvalArena#">
    <img src="https://img.shields.io/badge/Voice%20Anonymization%20Eval-black?logo=github">
  </a> 


# ğŸ™ï¸ SynEvalArena

Following our release of the [**Voice Privacy Arena**](https://github.com/Speech-Arena/SynEvalArena#) ğŸ† leaderboard, the **Speech Arena Team** proudly presents the accompanying **SynEvalArena Toolkit** to help you evaluate your own systems with ease.  

---

# ğŸ§­ Overview

Traditional evaluation of voice anonymization systems focuses mainly on metrics such as **EER (Equal Error Rate)** and **WER (Word Error Rate)**.  
In addition to these, the toolkit provides a broader suite of metrics âš™ï¸ for robust, multi-dimensional evaluation of **Voice Anonymization (VA)** systems.

This repository contains two main toolkit directories ğŸ“:

- ğŸ§¬ **`synthetic_speech_eval`** â€“ Metrics for evaluating *synthetic speech* generated by modern end-to-end voice anonymization and voice conversion systems.  
  ğŸ“˜ See the [Readme](./synthetic_speech_eval/README.md) for detailed usage.

- ğŸ” **`voice_privacy_eval`** â€“ Introduces two new metrics:  
  **AUDI (Audio Utility Distortion Index)** and **Fusion EER**, designed measure **utility** and **privacy** by using multiple reference systems for robust evaluation.  
  ğŸ“˜ Refer to the [Readme](./voice_privacy_eval/README.md) for more details.

---


